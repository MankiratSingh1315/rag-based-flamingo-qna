{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF RAG with Session Management and Gemini API\\n\n",
        "This notebook allows you to upload PDF documents, process them for Retrieval Augmented Generation (RAG), and chat with an AI model (Gemini) based on the content of your documents. Sessions are used to manage data, and they expire after a set duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q sentence_transformers faiss-cpu PyPDF2 google-generativeai numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "import PyPDF2\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# --- Constants ---\n",
        "SESSION_DATA_DIR = \\\n",
        "\n",
        "UPLOAD_DIR_NAME = \\\n",
        "\n",
        "CHUNKS_FILE_NAME = \\\n",
        "\n",
        "INDEX_FILE_NAME = \\\n",
        "\n",
        "STATUS_FILE_NAME = \\\n",
        "\n",
        "TIMESTAMP_FILE_NAME = \\\n",
        "\n",
        "MAX_FILES = 20\n",
        "MAX_TOTAL_SIZE_MB = 1024  # 1 GB\n",
        "SESSION_EXPIRY_HOURS = 2\n",
        "\n",
        "# --- Gemini API Configuration ---\n",
        "# IMPORTANT: Set your GEMINI_API_KEY as an environment variable before running this notebook.\n",
        "# You can also set it directly here for testing, but environment variables are safer.\n",
        "GEMINI_API_KEY = os.environ.get(\\\n",
        ")\n",
        "gemini_model = None\n",
        "if GEMINI_API_KEY:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    gemini_model = genai.GenerativeModel('gemini-pro')\n",
        "else:\n",
        "    print(\\\n",
        "\n",
        "# --- Sentence Transformer Model ---\n",
        "try:\n",
        "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "except Exception as e:\n",
        "    print(f\\\n",
        "    sentence_model = None\n",
        "\n",
        "os.makedirs(SESSION_DATA_DIR, exist_ok=True)\n",
        "print(\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_session_path(session_id):\n",
        "    return os.path.join(SESSION_DATA_DIR, str(session_id))\n",
        "\n",
        "def get_session_upload_path(session_id):\n",
        "    return os.path.join(get_session_path(session_id), UPLOAD_DIR_NAME)\n",
        "\n",
        "def ensure_session_dirs(session_id):\n",
        "    session_path = get_session_path(session_id)\n",
        "    os.makedirs(session_path, exist_ok=True)\n",
        "    os.makedirs(get_session_upload_path(session_id), exist_ok=True)\n",
        "    with open(os.path.join(session_path, TIMESTAMP_FILE_NAME), \\\n",
        ") as f:\n",
        "        f.write(datetime.now().isoformat())\n",
        "    print(f\\\n",
        "\n",
        "def get_session_status(session_id):\n",
        "    session_path = get_session_path(session_id)\n",
        "    status_file = os.path.join(session_path, STATUS_FILE_NAME)\n",
        "    timestamp_file = os.path.join(session_path, TIMESTAMP_FILE_NAME)\n",
        "\n",
        "    if not os.path.exists(session_path) or not os.path.exists(timestamp_file):\n",
        "        return \\\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(timestamp_file, \\\n",
        ") as f:\n",
        "            created_time_str = f.read()\n",
        "        created_time = datetime.fromisoformat(created_time_str)\n",
        "        if datetime.now() > created_time + timedelta(hours=SESSION_EXPIRY_HOURS):\n",
        "            return \\\n",
        " \n",
        "    except Exception as e:\n",
        "        print(f\\\n",
        "        return \\\n",
        " # Indicates an issue with the session's age check\n",
        "\n",
        "    if os.path.exists(status_file):\n",
        "        with open(status_file, \\\n",
        ") as f:\n",
        "            return f.read().strip()\n",
        "    return \\\n",
        " # Default if status file not yet created but session exists\n",
        "\n",
        "def update_session_status(session_id, status):\n",
        "    session_path = get_session_path(session_id)\n",
        "    if not os.path.exists(session_path):\n",
        "        ensure_session_dirs(session_id) # Should ideally be created before status update\n",
        "    with open(os.path.join(session_path, STATUS_FILE_NAME), \\\n",
        ") as f:\n",
        "        f.write(status)\n",
        "    print(f\\\n",
        "\n",
        "def extract_text_from_pdf_in_chunks(pdf_path, char_limit=1000):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        chunks_data = []\n",
        "        current_chunk_text = \\\n",
        "\n",
        "        current_chunk_metadata = []\n",
        "        for page_num, page in enumerate(pdf_reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "            if not page_text: continue\n",
        "            page_sentences = page_text.replace('\\\\n', ' ').split('. ')\n",
        "            for sentence in page_sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if not sentence: continue\n",
        "                sentence += \\\n",
        "\n",
        "                if len(current_chunk_text) + len(sentence) <= char_limit:\n",
        "                    current_chunk_text += \\\n",
        " + sentence\n",
        "                    if not current_chunk_metadata or current_chunk_metadata[-1]['page_number'] != page_num + 1:\n",
        "                        current_chunk_metadata.append({'page_number': page_num + 1})\n",
        "                else:\n",
        "                    if current_chunk_text:\n",
        "                        chunks_data.append((current_chunk_text.strip(), [{'page_number': meta['page_number']} for meta in current_chunk_metadata]))\n",
        "                    current_chunk_text = sentence\n",
        "                    current_chunk_metadata = [{'page_number': page_num + 1}]\n",
        "        if current_chunk_text:\n",
        "            chunks_data.append((current_chunk_text.strip(), [{'page_number': meta['page_number']} for meta in current_chunk_metadata]))\n",
        "    return chunks_data\n",
        "\n",
        "def process_pdfs_for_session(session_id, local_sentence_model):\n",
        "    if not local_sentence_model:\n",
        "        print(\\\n",
        ")\n",
        "        update_session_status(session_id, \\\n",
        ")\n",
        "        return\n",
        "    update_session_status(session_id, \\\n",
        ")\n",
        "    session_upload_path = get_session_upload_path(session_id)\n",
        "    pdf_files = [os.path.join(session_upload_path, f) for f in os.listdir(session_upload_path) if f.lower().endswith(\\\n",
        ")]\n",
        "    if not pdf_files:\n",
        "        print(\\\n",
        ")\n",
        "        update_session_status(session_id, \\\n",
        ")\n",
        "        return\n",
        "    print(f\\\n",
        "    all_chunks = []\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\\\n",
        ")\n",
        "        try:\n",
        "            all_chunks.extend(extract_text_from_pdf_in_chunks(pdf_file))\n",
        "        except Exception as e:\n",
        "            print(f\\\n",
        "            update_session_status(session_id, f\\\n",
        ")\n",
        "            return\n",
        "    if not all_chunks:\n",
        "        print(\\\n",
        ")\n",
        "        update_session_status(session_id, \\\n",
        ")\n",
        "        return\n",
        "    print(f\\\n",
        ")\n",
        "    chunk_texts = [chunk[0] for chunk in all_chunks]\n",
        "    try:\n",
        "        chunk_embeddings = local_sentence_model.encode(chunk_texts, show_progress_bar=True)\n",
        "        chunk_embeddings_float32 = np.array(chunk_embeddings, dtype=np.float32)\n",
        "        index = faiss.IndexFlatL2(chunk_embeddings_float32.shape[1])\n",
        "        index.add(chunk_embeddings_float32)\n",
        "        session_path = get_session_path(session_id)\n",
        "        faiss.write_index(index, os.path.join(session_path, INDEX_FILE_NAME))\n",
        "        with open(os.path.join(session_path, CHUNKS_FILE_NAME), 'wb') as f:\n",
        "            pickle.dump(all_chunks, f)\n",
        "        update_session_status(session_id, \\\n",
        ")\n",
        "        print(\\\n",
        ")\n",
        "    except Exception as e:\n",
        "        print(f\\\n",
        "        update_session_status(session_id, \\\n",
        ")\n",
        "\n",
        "def retrieve_chunks_session(query, local_sentence_model, session_id, top_k=5):\n",
        "    if not local_sentence_model:\n",
        "        print(\\\n",
        ")\n",
        "        return []\n",
        "    session_path = get_session_path(session_id)\n",
        "    index_path = os.path.join(session_path, INDEX_FILE_NAME)\n",
        "    chunks_path = os.path.join(session_path, CHUNKS_FILE_NAME)\n",
        "    if not os.path.exists(index_path) or not os.path.exists(chunks_path):\n",
        "        print(\\\n",
        ")\n",
        "        return []\n",
        "    index = faiss.read_index(index_path)\n",
        "    with open(chunks_path, 'rb') as f:\n",
        "        chunks = pickle.load(f)\n",
        "    query_embedding = local_sentence_model.encode([query])\n",
        "    D, I = index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
        "    retrieved_chunks_data = []\n",
        "    for i, idx in enumerate(I[0]):\n",
        "        if 0 <= idx < len(chunks):\n",
        "            chunk_text = chunks[idx][0]\n",
        "            page_info = f\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
